import os
import numpy as np
import ray
import torch
import torch.nn as nn
import supersuit as ss
import imageio.v2 as imageio
import warnings

from ray.rllib.algorithms.algorithm import Algorithm
from ray.rllib.models import ModelCatalog
from ray.rllib.models.torch.torch_modelv2 import TorchModelV2
from ray.tune.registry import register_env
from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv
from pettingzoo.butterfly import cooperative_pong_v5

# ì‚¬ìš©ìê°€ ì •ì˜í•œ Wrapper (íŒŒì¼ì´ ê°™ì€ ê²½ë¡œì— ìˆì–´ì•¼ í•¨)
from RewardShapingWrapper import RewardShapingWrapper
from MirrorObservationWrapper import MirrorObservationWrapper

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore", category=DeprecationWarning)

# ==============================================================================
# 1. Custom Model Definition (í•™ìŠµ ì½”ë“œì™€ ë™ì¼í•´ì•¼ í•¨)
# ==============================================================================
class CustomCNN(TorchModelV2, nn.Module):
    def __init__(self, obs_space, action_space, num_outputs, model_config, name):
        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)
        nn.Module.__init__(self)

        shape = obs_space.shape
        input_channels = shape[2] if len(shape) == 3 else 1
        input_h, input_w = shape[:2]

        self.conv_layers = nn.Sequential(
            nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),
            nn.ReLU()
        )

        dummy_input = torch.zeros(1, input_channels, input_h, input_w)
        with torch.no_grad():
            self.flatten_dim = self.conv_layers(dummy_input).numel()

        self.fc_net = nn.Sequential(
            nn.Flatten(),
            nn.Linear(self.flatten_dim, 512),
            nn.ReLU()
        )

        self.policy_head = nn.Linear(512, num_outputs)
        self.value_head = nn.Linear(512, 1)
        self._value_out = None

    def forward(self, input_dict, state, seq_lens):
        x = input_dict["obs"].float()
        if x.max() > 10.0: x = x / 255.0
        if x.dim() == 3: x = x.unsqueeze(-1)
        x = x.permute(0, 3, 1, 2)

        x = self.fc_net(self.conv_layers(x))
        logits = self.policy_head(x)
        self._value_out = self.value_head(x).squeeze(-1)
        return logits, state

    def value_function(self):
        return self._value_out

# ëª¨ë¸ ë“±ë¡
ModelCatalog.register_custom_model("custom_cnn", CustomCNN)


# ==============================================================================
# 2. Environment Setup (í•™ìŠµ ì „ì²˜ë¦¬ ê³¼ì •ê³¼ ë™ì¼í•´ì•¼ í•¨)
# ==============================================================================
def make_eval_env(render_mode="rgb_array", max_cycles=500):
    """
    GIF ìƒì„±ì„ ìœ„í•œ í™˜ê²½ ìƒì„± í•¨ìˆ˜.
    í•™ìŠµ ë•Œ ì‚¬ìš©í•œ ì „ì²˜ë¦¬(Resize ë“±)ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•¨.
    """
    env = cooperative_pong_v5.parallel_env(max_cycles=max_cycles, render_mode=render_mode)
    
    # [ì¤‘ìš”] í•™ìŠµ ì½”ë“œì— ìˆë˜ ë¦¬ì‚¬ì´ì§• (168x84) ì ìš©
    env = ss.resize_v1(env, x_size=168, y_size=84)
    
    # Wrapper ì ìš©
    env = RewardShapingWrapper(env)
    env = MirrorObservationWrapper(env)
    
    return env

def register_rllib_env(env_name="coop_pong_eval"):
    """
    RLlib Algorithm ë¡œë”© ì‹œ í™˜ê²½ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ë“±ë¡í•˜ëŠ” í•¨ìˆ˜
    """
    def env_creator(config):
        return ParallelPettingZooEnv(make_eval_env(render_mode=None))
    
    register_env(env_name, env_creator)


# ==============================================================================
# 3. Main Logic
# ==============================================================================
def rollout_checkpoint_to_gif(
    checkpoint_path: str,
    out_gif_path: str,
    num_episodes: int = 1,
    max_cycles: int = 500,
    fps: int = 30,
):
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")

    # 1) Ray ì´ˆê¸°í™”
    if not ray.is_initialized():
        ray.init(ignore_reinit_error=True, include_dashboard=False)

    # 2) ëª¨ë¸ ë° í™˜ê²½ ë“±ë¡ (ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì „ í•„ìˆ˜)
    ModelCatalog.register_custom_model("custom_cnn", CustomCNN)
    register_rllib_env("cooperative_pong_shared_DoubleDQN") # í•™ìŠµ ì½”ë“œì˜ í™˜ê²½ ì´ë¦„ ì‚¬ìš©

    print(f"Loading checkpoint from: {checkpoint_path}")
    
    # 3) ì•Œê³ ë¦¬ì¦˜ ë³µì›
    # í™˜ê²½ ì´ë¦„ì´ configì— ì €ì¥ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, ìœ„ì—ì„œ ë“±ë¡í•œ ì´ë¦„ê³¼ ì¼ì¹˜í•´ì•¼ í•¨
    algo = Algorithm.from_checkpoint(checkpoint_path)

    # 4) ë Œë”ë§ìš© ì‹¤ì œ í™˜ê²½ ìƒì„±
    env = make_eval_env(render_mode="rgb_array", max_cycles=max_cycles)

    frames = []

    print("Starting rollout...")
    for ep in range(num_episodes):
        obs, infos = env.reset()
        
        # ì²« í”„ë ˆì„ ë Œë”ë§
        fr = env.render()
        if fr is not None: frames.append(fr)

        terminations = {a: False for a in env.possible_agents}
        truncations = {a: False for a in env.possible_agents}
        
        step_i = 0
        while True:
            # ëª¨ë“  ì—ì´ì „íŠ¸ ì¢…ë£Œ ì²´í¬
            if all(terminations.get(a, False) or truncations.get(a, False) for a in env.possible_agents):
                break
                
            actions = {}
            for agent_id, agent_obs in obs.items():
                # [ì¤‘ìš”] explore=Falseë¡œ ì„¤ì •í•˜ì—¬ í•™ìŠµëœ ìµœì  í–‰ë™ë§Œ ì„ íƒ (Epsilon=0)
                action = algo.compute_single_action(
                    agent_obs, 
                    policy_id="shared_policy", 
                    explore=False 
                )
                actions[agent_id] = action

            obs, rewards, terminations, truncations, infos = env.step(actions)

            # í”„ë ˆì„ ì €ì¥
            fr = env.render()
            if fr is not None: frames.append(fr)
            
            step_i += 1
            if step_i >= max_cycles:
                break
        
        print(f"Episode {ep+1} finished with {step_i} steps.")

    env.close()

    # 5) GIF ì €ì¥
    if frames:
        os.makedirs(os.path.dirname(out_gif_path) or ".", exist_ok=True)
        imageio.mimsave(out_gif_path, frames, fps=fps)
        print(f"âœ… Success! Saved GIF to: {out_gif_path} (Total frames: {len(frames)})")
    else:
        print("âŒ Error: No frames captured.")

    algo.stop()
    ray.shutdown()

if __name__ == "__main__":
    # ==========================================================================
    # ğŸ‘‡ ê²½ë¡œ ì„¤ì • ë¶€ë¶„ (ì—¬ê¸°ë¥¼ ë³¸ì¸ì˜ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”)
    # ì˜ˆ: "results/DoubleDQN.../DQN_.../checkpoint_000100" í´ë” ê²½ë¡œ
    # ==========================================================================
    
    # ì˜ˆì‹œ ê²½ë¡œ (ë³¸ì¸ì˜ ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½ í•„ìš”)
    CHECKPOINT_PATH = "/home/jsr/project/Cooperative_pong_RL_agent/checkpoint_best"
    
    OUTPUT_GIF = "checkpoint_result3.gif"

    # ì‹¤í–‰
    # (ì£¼ì˜: ê²½ë¡œê°€ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ FileNotFoundError ë°œìƒ)
    try:
        rollout_checkpoint_to_gif(
            checkpoint_path=CHECKPOINT_PATH,
            out_gif_path=OUTPUT_GIF,
            num_episodes=1,
            max_cycles=500,
            fps=30
        )
    except Exception as e:
        print(f"\n[Error Occurred] {e}")
        print("Check if the CHECKPOINT_PATH is correct.")
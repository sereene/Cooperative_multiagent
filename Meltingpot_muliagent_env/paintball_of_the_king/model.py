import numpy as np
import torch
import torch.nn as nn
from ray.rllib.models.torch.torch_modelv2 import TorchModelV2
from ray.rllib.utils.annotations import override

class MeltingPotModel(TorchModelV2, nn.Module):
    """
    LSTM ì—†ì´ FrameStackì„ ì‚¬ìš©í•˜ëŠ” ê°€ë²¼ìš´ CNN ëª¨ë¸
    - íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° ê¸°ëŠ¥ ì¶”ê°€
    - CNN ì¶œë ¥ í¬ê¸° ìžë™ ê³„ì‚° ê¸°ëŠ¥ ì¶”ê°€ (stride ë³€ê²½ ëŒ€ì‘)
    """
    def __init__(self, obs_space, action_space, num_outputs, model_config, name):
        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)
        nn.Module.__init__(self)

        in_channels = obs_space.shape[2] 

        # CNN Layers 
        self.conv_layers = nn.Sequential(
            # [88,88,12] -> [11,11,16]
            nn.Conv2d(in_channels, 16, kernel_size=8, stride=8),
            nn.ReLU(),
            nn.Conv2d(16, 32, kernel_size=4, stride=1), 
            nn.ReLU(),
            nn.Flatten()
        )
        
        # CNN ì¶œë ¥ í¬ê¸° ê³„ì‚°
        with torch.no_grad():
            # (Batch=1, C, H, W) ë”ë¯¸ ìž…ë ¥ ìƒì„±
            dummy_input = torch.zeros(1, in_channels, 88, 88) 
            cnn_out = self.conv_layers(dummy_input)
            flatten_size = cnn_out.numel()

        # MLP Layers
        self.fc1 = nn.Linear(flatten_size, 128)
        self.fc2 = nn.Linear(128, 128)

        # heads
        self.policy_head = nn.Linear(128, num_outputs)
        self.value_head = nn.Linear(128, 1)
        
        self._features = None

        # ----------------------------------------------------------------
        # [ìš”ì²­í•˜ì‹  ê¸°ëŠ¥] í•™ìŠµì— ì°¸ì—¬í•˜ëŠ” í™œì„±í™”ëœ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° ë° ì¶œë ¥
        # ----------------------------------------------------------------
        
        # 1) CNN íŒŒë¼ë¯¸í„° ìˆ˜
        cnn_params = sum(p.numel() for p in self.conv_layers.parameters() if p.requires_grad)
        
        # 2) ì „ì²´ ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ (CNN + MLP + Heads)
        total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        print("="*50)
        print(f"ðŸ¤– [Model Info] Trainable Parameters Count")
        print(f"   1. After CNN Layers : {cnn_params:,} parameters")
        print(f"   2. Final Total Model: {total_params:,} parameters")
        print("="*50)

    @override(TorchModelV2)
    def forward(self, input_dict, state, seq_lens):
        # 1. ìž…ë ¥ ê°€ì ¸ì˜¤ê¸°
        x = input_dict["obs"]
        
        # ì •ê·œí™”
        x = x.float() / 255.0
        
        # 2. ì°¨ì› ë³€í™˜: [B, H, W, C] -> [B, C, H, W]
        x = x.permute(0, 3, 1, 2)

        # 3. CNN í†µê³¼
        x = self.conv_layers(x)
        
        # 4. MLP í†µê³¼
        x = torch.relu(self.fc1(x))
        self._features = torch.relu(self.fc2(x))
        
        # 5. Output
        logits = self.policy_head(self._features)
        
        return logits, []

    @override(TorchModelV2)
    def value_function(self):
        assert self._features is not None, "must call forward() first"
        return self.value_head(self._features).reshape(-1)
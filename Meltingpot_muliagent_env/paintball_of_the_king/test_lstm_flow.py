import torch
import torch.nn as nn
import numpy as np
import gymnasium as gym
from CNN_LSTM_model import MeltingPotModel

def test_meltingpot_lstm():
    print("="*60)
    print("üß™ [Test] MeltingPot LSTM Model Integrity Check")
    print("="*60)

    # ---------------------------------------------------------
    # 1. ÌôòÍ≤Ω Î∞è Î™®Îç∏ ÏÑ§Ï†ï (Mocking)
    # ---------------------------------------------------------
    # Í∞ÄÏßú Í¥ÄÏ∏° Í≥µÍ∞Ñ (88x88 RGB Ïù¥ÎØ∏ÏßÄ)
    obs_space = gym.spaces.Box(0, 255, shape=(88, 88, 3), dtype=np.uint8)
    
    # Í∞ÄÏßú ÌñâÎèô Í≥µÍ∞Ñ (Discrete 8)
    action_space = gym.spaces.Discrete(8)
    
    # Î™®Îç∏ Ï¥àÍ∏∞Ìôî
    model_config = {"custom_model_config": {}}
    model = MeltingPotModel(
        obs_space=obs_space,
        action_space=action_space,
        num_outputs=8,
        model_config=model_config,
        name="test_model"
    )
    
    print(f"‚úÖ Model Initialized: {model}")

    # ---------------------------------------------------------
    # 2. ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± (Batch=2, Time=5)
    # ---------------------------------------------------------
    B, T = 2, 5
    input_dict = {
        "obs": torch.randint(0, 255, (B, T, 88, 88, 3), dtype=torch.float32), # Ïù¥ÎØ∏ÏßÄ
        "prev_actions": torch.randint(0, 8, (B, T)).long(),
        "prev_rewards": torch.randn(B, T)
    }
    
    # Ï¥àÍ∏∞ ÏÉÅÌÉú Í∞ÄÏ†∏Ïò§Í∏∞ (h, c)
    state = model.get_initial_state()
    # Batch ÌÅ¨Í∏∞Ïóê ÎßûÍ≤å ÏÉÅÌÉú ÌôïÏû• (RLLib ÎÇ¥Î∂Ä ÎèôÏûë Î™®Î∞©)
    # stateÎäî [Hidden_State, Cell_State] Î¶¨Ïä§Ìä∏
    # Í∞Å ÌÖêÏÑúÎäî [Batch, Hidden_Size] Ïó¨Ïïº Ìï®
    state = [s.unsqueeze(0).repeat(B, 1) for s in state] 
    
    seq_lens = torch.LongTensor([T] * B) # Î™®Îì† ÏãúÌÄÄÏä§ Í∏∏Ïù¥Îäî 5

    print(f"\nüìä Input Shape: {input_dict['obs'].shape} (Batch={B}, Time={T})")
    print(f"üìä Initial State Shape: h={state[0].shape}, c={state[1].shape}")

    # ---------------------------------------------------------
    # 3. Forward Pass (Ï†ÑÌåå)
    # ---------------------------------------------------------
    output, new_state = model(input_dict, state, seq_lens)
    
    # Í≤ÄÏ¶ù 1: Ï∂úÎ†• ÌÅ¨Í∏∞ ÌôïÏù∏
    expected_shape = (B * T, 8) # [10, 8]
    assert output.shape == expected_shape, f"‚ùå Output shape mismatch! Expected {expected_shape}, got {output.shape}"
    print(f"‚úÖ Forward Pass Successful. Output Shape: {output.shape}")

    # Í≤ÄÏ¶ù 2: ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏ ÌôïÏù∏ (Í∏∞ÏñµÏù¥ Î≥ÄÌñàÎäîÍ∞Ä?)
    # Ï¥àÍ∏∞ ÏÉÅÌÉú(0)ÏôÄ ÏÉà ÏÉÅÌÉúÍ∞Ä Îã¨ÎùºÏïº Ìï®
    is_state_updated = not torch.allclose(state[0], new_state[0])
    if is_state_updated:
        print("‚úÖ LSTM State Updated: Memory is changing based on input.")
    else:
        print("‚ùå Warning: LSTM State did NOT change. (Check if inputs are all zero or gradients are disconnected)")

    # ---------------------------------------------------------
    # 4. Backward Pass (ÌïôÏäµ Ïã†Ìò∏ Ï†ÑÎã¨)
    # ---------------------------------------------------------
    # ÏûÑÏùòÏùò ÏÜêÏã§ Ìï®Ïàò Í≥ÑÏÇ∞ (Mean Squared Error)
    target = torch.randn_like(output)
    loss = nn.functional.mse_loss(output, target)
    
    # Ïó≠Ï†ÑÌåå
    loss.backward()
    
    # LSTM Í∞ÄÏ§ëÏπòÏóê GradientÍ∞Ä Îß∫ÌòîÎäîÏßÄ ÌôïÏù∏
    lstm_weight_grad = model.lstm.weight_ih_l0.grad
    if lstm_weight_grad is not None and torch.sum(torch.abs(lstm_weight_grad)) > 0:
        grad_norm = torch.norm(lstm_weight_grad).item()
        print(f"‚úÖ Gradient Flow Confirmed! (LSTM Weight Grad Norm: {grad_norm:.4f})")
    else:
        print("‚ùå Error: No gradient flow to LSTM. The model will not learn.")

    
    print("\nüéâ Test Complete. If all checks passed, your LSTM implementation is correct.")

if __name__ == "__main__":
    try:
        test_meltingpot_lstm()
    except Exception as e:
        print(f"\n‚ùå Test Failed with Exception: {e}")
        import traceback
        traceback.print_exc()
